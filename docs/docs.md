---
layout: default
title: Documentation
charset: utf-8
---

# AI Multi-Agent Toolkit Documentation

AI Multi-Agent Toolkitì˜ ìƒì„¸ ì‚¬ìš©ë²•ê³¼ ì˜ˆì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## 1. ì—…ë°ì´íŠ¸ í˜„í™©

í˜„ì¬ê¹Œì§€ êµ¬í˜„ëœ ê¸°ëŠ¥ì— ëŒ€í•œ ìš”ì•½ì…ë‹ˆë‹¤.

- LLM ì—ì´ì „íŠ¸ ìƒì„± ë° ì‘ë‹µ ë°›ê¸° (ollama, openai, genai(gemini) ì§€ì›)
- Discordë¡œ ë©”ì‹œì§€ ë³´ë‚´ê¸° (ë©”ì‹œì§€ ì²­í¬ ë¶„í•  ë° ê²¹ì¹¨ ì§€ì›)
- ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê¸° (whisper ì‚¬ìš©)
- í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ë° ê²¹ì¹¨ ì§€ì›
- 2025/09/17 ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ êµ¬í˜„
- 2025/09/21 LLM_Agent í´ë˜ìŠ¤ì— __call__ ë§¤ì§ ë©”ì„œë“œê°€ ì¶”ê°€ë˜ì–´ agent.generate_response() ëŒ€ì‹  agent() ì§ì ‘ í˜¸ì¶œì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.
- 2025/09/21 LLM_Agent ë©€í‹°ëª¨ë‹¬ ì‚¬ìš© ì¶”ê°€
- 2025/09/22 ë©€í‹° ì—ì´ì „íŠ¸ ì‹œí—˜ ë¬¸ì œ ìƒì„±ê¸° ì˜ˆì œ ì¶”ê°€ (YouTube â†’ ì‹œí—˜ ë¬¸ì œ ìë™ ìƒì„±)
- 2025/09/23 ë©”ëª¨ë¦¬ ì €ì¥ ë¡œì§ ê°œì„  (ì‹¤ì œ LLM ì „ì†¡ ì»¨í…ìŠ¤íŠ¸ì™€ ë©”ëª¨ë¦¬ ì €ì¥ ë‚´ìš© ì¼ì¹˜í™”)

## 2. ëª¨ë“ˆ ì†Œê°œ

### LLM ì—ì´ì „íŠ¸ í•µì‹¬ ëª¨ë“ˆ

#### 1) ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - ğŸ“„ llm_agent.py

```python
# ê¸°ë³¸ LLM ì—ì´ì „íŠ¸
from module.llm_agent import LLM_Agent

agent = LLM_Agent(
    model_name="gemini-2.5-flash",
    provider="genai",  # ollama, genai, openai
    api_key="your_api_key",
    session_id="chat_session",
    max_history=10
)

# ë©€í‹°ëª¨ë‹¬ ì—ì´ì „íŠ¸
from module.llm_agent import Multi_modal_agent

modal_agent = Multi_modal_agent(
    model_name="gemma3:12b",
    provider="ollama",  # ollama, genai
    api_key=None
)
```

**ì§€ì› ê¸°ëŠ¥:** ë‹¤ì¤‘ LLM í”„ë¡œë°”ì´ë” ì§€ì› (Ollama, OpenAI, Google Gemini), ëŒ€í™” ê¸°ì–µ ê¸°ëŠ¥, ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—…, ì´ë¯¸ì§€ ë¶„ì„ ë° OCR (ë©€í‹°ëª¨ë‹¬), __call__ ë§¤ì§ ë©”ì„œë“œë¡œ ê°„í¸í•œ í˜¸ì¶œ

#### 2) ì—ì´ì „íŠ¸ í˜¸ì¶œí•˜ê³  ì‘ë‹µë°›ê¸°

```python
# ê¸°ë³¸ í…ìŠ¤íŠ¸ ëŒ€í™”
response = agent(
    system_prompt="ë‹¹ì‹ ì€ ì¹œê·¼í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.",
    user_message="ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?"
)
print(response)  # AIì˜ ë‹µë³€ì´ ì¶œë ¥ë©ë‹ˆë‹¤

# ë©”ëª¨ë¦¬ ê¸°ëŠ¥ìœ¼ë¡œ ì´ì „ ëŒ€í™” ê¸°ì–µí•˜ê¸°
response_with_memory = agent(
    system_prompt="ë‹¹ì‹ ì€ ì¹œê·¼í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.",
    user_message="ì œ ì´ë¦„ì„ ê¸°ì–µí•˜ì‹œë‚˜ìš”?",
    memory=True  # ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤
)

# ì´ë¯¸ì§€ì™€ í•¨ê»˜ ëŒ€í™”í•˜ê¸° (ë©€í‹°ëª¨ë‹¬)
image_response = modal_agent(
    system_prompt="ë‹¹ì‹ ì€ ì´ë¯¸ì§€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
    user_message="ì´ ì‚¬ì§„ì— ë­ê°€ ë³´ì´ë‚˜ìš”?",
    image_path="my_photo.jpg"  # ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼
)
print(image_response)  # ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ê°€ ì¶œë ¥ë©ë‹ˆë‹¤
```

### Discord ì›¹í›… ì—°ë™ ëª¨ë“ˆ

#### Discord ë©”ì‹œì§€ ì „ì†¡ - ğŸ“„ discord.py

```python
from module.discord import Send_to_discord

discord = Send_to_discord(
    base_url="https://discord.com/api/webhooks/your_webhook_url"
)

# ê¸°ë³¸ ë©”ì‹œì§€ ì „ì†¡
discord.send_message("ì•ˆë…•í•˜ì„¸ìš”! ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤.")

# ê¸´ í…ìŠ¤íŠ¸ ìë™ ë¶„í•  ì „ì†¡
long_text = "ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸..." * 1000
discord.send_message(long_text)  # ìë™ìœ¼ë¡œ 2000ìì”© ë¶„í• í•´ì„œ ì „ì†¡
```

**ì£¼ìš” ê¸°ëŠ¥:** LLM ì‘ë‹µì„ Discord ì±„ë„ë¡œ ìë™ ì „ì†¡, ê¸´ ë©”ì‹œì§€ ì²­í¬ ë¶„í•  ì²˜ë¦¬

### ìŒì„± ì²˜ë¦¬ ëª¨ë“ˆ

#### ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ - ğŸ“„ audio_tool.py

```python
from module.audio_tool import Audio_to_text

# ìŒì„± ë³€í™˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
audio_converter = Audio_to_text()

# ë‹¨ì¼ íŒŒì¼ ë³€í™˜
result = audio_converter.convert_single_file("my_audio.mp3")
print(result)  # ë³€í™˜ëœ í…ìŠ¤íŠ¸ê°€ ì¶œë ¥ë©ë‹ˆë‹¤

# í´ë” ë‚´ ëª¨ë“  ì˜¤ë””ì˜¤ íŒŒì¼ ì¼ê´„ ë³€í™˜
audio_converter.convert_folder("audio_files/")
```

**ì£¼ìš” ê¸°ëŠ¥:** Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜, ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í¬ë§· ì§€ì›, ë°°ì¹˜ ì²˜ë¦¬

### ëŒ€í™” ê¸°ì–µ ê´€ë¦¬ ëª¨ë“ˆ

#### ëŒ€í™” ê¸°ë¡ SQLite ì €ì¥ - ğŸ“„ memory.py

```python
from module.memory import MemoryManager

memory = MemoryManager(db_path="conversations.db")

# ëŒ€í™” ê¸°ë¡ ì €ì¥
memory.save_history("session_1", [
    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"},
    {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}
])

# ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
history = memory.get_history("session_1")
print(history)
```

**ì£¼ìš” ê¸°ëŠ¥:** LLM ì—ì´ì „íŠ¸ì˜ ëŒ€í™” ê¸°ë¡ì„ SQLiteì— ì €ì¥í•˜ê³  ê´€ë¦¬ (ì±—ë´‡ ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì— ì‚¬ìš©)

### í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°

#### ê¸´ í…ìŠ¤íŠ¸ ì²­í‚¹ ë¶„í•  - ğŸ“„ text_tool.py

```python
from module.text_tool import Text_tool

text_tool = Text_tool(
    chunk_size=1000,
    overlap=100
)

# ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
chunks = text_tool.split_text("ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ ë‚´ìš©...")
print(f"ì´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨")

# ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ìë™ ì €ì¥
text_tool.save_to_json(chunks, "output.json")
```

**ì£¼ìš” ê¸°ëŠ¥:** ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• , JSON ê²°ê³¼ ìë™ ì €ì¥ (Discord ë©”ì‹œì§€ ì „ì†¡, ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬, ë©€í‹° ì—ì´ì „íŠ¸ ê²°ê³¼ ì €ì¥ì— ì‚¬ìš©)

## 3. LLM ì—ì´ì „íŠ¸ ì‚¬ìš©í•˜ê¸°

### ê¸°ë³¸ ì—ì´ì „íŠ¸ ìƒì„± ë° ëŒ€í™”

```python
from module.llm_agent import LLM_Agent

# Ollama ì‚¬ìš© (ë¡œì»¬ í™˜ê²½)
agent = LLM_Agent(model_name="gemma3", provider="ollama")

response = agent(
    system_prompt="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤.",
    user_message="Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
)
print(response)
```

### ë©”ëª¨ë¦¬ ê¸°ëŠ¥ìœ¼ë¡œ ì—°ì† ëŒ€í™”

```python
# ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ ìˆëŠ” ì—ì´ì „íŠ¸
memory_agent = LLM_Agent(
    model_name="gpt-4o-mini",
    provider="openai",
    api_key="your_openai_key",
    session_id="my_conversation",
    max_history=20
)

# ì²« ë²ˆì§¸ ëŒ€í™”
response1 = memory_agent(
    system_prompt="ë‹¹ì‹ ì€ ì¹œê·¼í•œ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.",
    user_message="ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤. ê¸°ì–µí•´ì£¼ì„¸ìš”.",
    memory=True
)

# ë‘ ë²ˆì§¸ ëŒ€í™” (ì´ì „ ë‚´ìš©ì„ ê¸°ì–µí•¨)
response2 = memory_agent(
    system_prompt="ë‹¹ì‹ ì€ ì¹œê·¼í•œ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.",
    user_message="ì œ ì´ë¦„ì´ ë­ì˜€ì£ ?",
    memory=True
)
print(response2)  # "ê¹€ì² ìˆ˜ë‹˜"ì´ë¼ê³  ë‹µë³€í•  ê²ƒì…ë‹ˆë‹¤
```

### ë©€í‹°ëª¨ë‹¬ ì—ì´ì „íŠ¸ë¡œ ì´ë¯¸ì§€ ë¶„ì„

```python
from module.llm_agent import Multi_modal_agent

# ì´ë¯¸ì§€ ë¶„ì„ ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸
vision_agent = Multi_modal_agent(
    model_name="gemini-2.5-flash",
    provider="genai",
    api_key="your_gemini_key"
)

# ì´ë¯¸ì§€ OCR ë° ë¶„ì„
result = vision_agent(
    system_prompt="ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.",
    user_message="ì´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”.",
    image_path="document.jpg"
)
print(result)
```

## 4. ë©€í‹° ì—ì´ì „íŠ¸ ì‹œí—˜ ë¬¸ì œ ìƒì„±ê¸°

YouTube ì˜ìƒì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ì‹œí—˜ ë¬¸ì œë¥¼ ìƒì„±í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.

```python
import os
from dotenv import load_dotenv
from module.llm_agent import LLM_Agent
import yt_dlp

load_dotenv()

# ì—ì´ì „íŠ¸ ì„¤ì •
agent = LLM_Agent(
    model_name="gemini-2.5-flash",
    provider="genai",
    api_key=os.getenv("GOOGLE_API_KEY")
)

# YouTube ì˜ìƒ ë‹¤ìš´ë¡œë“œ ë° ë¶„ì„
def generate_quiz_from_youtube(video_url):
    # ì˜ìƒ ì •ë³´ ì¶”ì¶œ
    with yt_dlp.YoutubeDL({'writeinfojson': True}) as ydl:
        info = ydl.extract_info(video_url, download=False)
        title = info.get('title', 'Unknown')
        description = info.get('description', '')

    # ë¬¸ì œ ìƒì„± ì—ì´ì „íŠ¸
    quiz_response = agent(
        system_prompt="ë‹¹ì‹ ì€ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ íš¨ê³¼ê°€ ë†’ì€ ì‹œí—˜ ë¬¸ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.",
        user_message=f"ì œëª©: {title}\nì„¤ëª…: {description[:2000]}\n\nìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì‹ 5ë¬¸í•­ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
    )

    return quiz_response

# ì‚¬ìš© ì˜ˆì‹œ
quiz = generate_quiz_from_youtube("https://youtube.com/watch?v=example")
print(quiz)
```

## 5. Discordë¡œ ë©”ì‹œì§€ ë³´ë‚´ê¸°

LLM ì‘ë‹µì„ Discord ì±„ë„ë¡œ ìë™ ì „ì†¡í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```python
from module.discord import Send_to_discord
from module.llm_agent import LLM_Agent

# ì—ì´ì „íŠ¸ì™€ Discord ì„¤ì •
agent = LLM_Agent(model_name="gemma3", provider="ollama")
discord = Send_to_discord(base_url="your_discord_webhook_url")

# LLM ì‘ë‹µì„ Discordë¡œ ì „ì†¡
system_prompt = "ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ë¹„ì„œì…ë‹ˆë‹¤."
user_prompt = "ì˜¤ëŠ˜ì˜ ë‚ ì”¨ì™€ ì¶”ì²œ í™œë™ì„ ì•Œë ¤ì£¼ì„¸ìš”."

response = agent(system_prompt, user_prompt)
discord.send_message(f"ğŸ¤– AI ì‘ë‹µ:\n{response}")
```

#### Discord ë©”ì‹œì§€ ì „ì†¡ ê³ ê¸‰ ì„¤ì •

```python
from module.discord import Send_to_discord

discord = Send_to_discord(
    base_url="your_discord_webhook_url",  # Discord ì›¹í›… URL
    chunk_size=1900,  # ë©”ì‹œì§€ ì²­í¬ í¬ê¸° (ê¸°ë³¸: 1900)
    overlap=0         # ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸° (ê¸°ë³¸: 0)
)

# ê¸´ ë©”ì‹œì§€ ìë™ ë¶„í•  ì „ì†¡
long_response = agent(system_prompt, "ë§¤ìš° ê¸´ ë‹µë³€ì´ í•„ìš”í•œ ì§ˆë¬¸")
discord.send_message(long_response)  # ìë™ìœ¼ë¡œ ì—¬ëŸ¬ ë©”ì‹œì§€ë¡œ ë¶„í•  ì „ì†¡
```

### ìŒì„± ì²˜ë¦¬ ëª¨ë“ˆ ê³ ê¸‰ ì‚¬ìš©ë²•

#### ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ - ğŸ“„ audio_tool.py

```python
from module.audio_tool import Audio

audio = Audio(
    text_output="transcripts",    # í…ìŠ¤íŠ¸ ì¶œë ¥ í´ë” (ê¸°ë³¸: "text")
    source_file="audio_files",    # ìŒì„± íŒŒì¼ í´ë” (ê¸°ë³¸: "source_file")
    whisper_model="large-v3",     # Whisper ëª¨ë¸ (ê¸°ë³¸: "large-v3")
    preferred_codec="mp3",        # ì„ í˜¸ ì˜¤ë””ì˜¤ ì½”ë± (ê¸°ë³¸: "mp3")
    preferred_quality="192"       # ì˜¤ë””ì˜¤ í’ˆì§ˆ (ê¸°ë³¸: "192")
)

# ë‹¨ì¼ íŒŒì¼ ë³€í™˜
result = audio.convert_single_file("my_audio.wav")
print(result)

# í´ë” ë‚´ ëª¨ë“  íŒŒì¼ ì¼ê´„ ë³€í™˜
audio.convert_folder_files()
```

### ëŒ€í™” ê¸°ì–µ ê´€ë¦¬ ëª¨ë“ˆ ìƒì„¸

#### ëŒ€í™” ê¸°ë¡ SQLite ì €ì¥ - ğŸ“„ memory.py

```python
from module.memory import MemoryManager

memory = MemoryManager(
    db_path="memory.db",           # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸: "memory.db")
    session_id="user_session_01",  # ì„¸ì…˜ ID (ê¸°ë³¸: None)
    messages=[]                    # ì´ˆê¸° ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: [])
)

# ëŒ€í™” ì €ì¥
memory.save_history("session_1", [
    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"},
    {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”."}
])

# ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°
history = memory.get_history("session_1")
for msg in history:
    print(f"{msg['role']}: {msg['content']}")
```

### í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ìƒì„¸

#### ê¸´ í…ìŠ¤íŠ¸ ì²­í‚¹ ë¶„í•  ê³ ê¸‰ ì„¤ì •

```python
from module.text_tool import Text_tool

text_tool = Text_tool(
    chunk_size=1000,  # ì²­í¬ í¬ê¸° (ê¸°ë³¸: 1000)
    overlap=100,      # ì²­í¬ ê°„ ê²¹ì¹¨ (ê¸°ë³¸: 0)
    max_length=5000   # ìµœëŒ€ ê¸¸ì´ ì œí•œ (ê¸°ë³¸: None)
)

# ê¸´ í…ìŠ¤íŠ¸ ë¶„í• 
long_text = "ë§¤ìš° ê¸´ ë¬¸ì„œ ë‚´ìš©..." * 1000
chunks = text_tool.split_text(long_text)
print(f"ì´ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨")

# ê° ì²­í¬ ì²˜ë¦¬
for i, chunk in enumerate(chunks):
    print(f"ì²­í¬ {i+1}: {len(chunk)}ì")
```

#### JSON ê²°ê³¼ ì €ì¥ ê¸°ëŠ¥

```python
# JSON ê²°ê³¼ ìë™ ì €ì¥ ê¸°ëŠ¥
text_tool = Text_tool()

# LLM ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
final_output = {
    "concepts_and_terms": ["ê´€ê³„ëŒ€ìˆ˜", "ì¡°ì¸", "ì°¨ì§‘í•©"],
    "questions": [
        {
            "question": "ê´€ê³„ëŒ€ìˆ˜ì—ì„œ 'ë³´íƒ€ì´ê¸°í˜¸'ëŠ” ì–´ë–¤ ì—°ì‚°ìë¥¼ ì˜ë¯¸í•˜ë‚˜ìš”?",
            "options": ["ì…€ë ‰íŠ¸", "í”„ë¡œì íŠ¸", "ì¡°ì¸", "ë””ë¹„ì „"],
            "answer": "ì¡°ì¸",
            "explanation": "ë³´íƒ€ì´ê¸°í˜¸ëŠ” ê´€ê³„ëŒ€ìˆ˜ì—ì„œ ì¡°ì¸ ì—°ì‚°ìë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."
        }
    ]
}

# ìë™ìœ¼ë¡œ final_outputs/quiz_result.json íŒŒì¼ë¡œ ì €ì¥
text_tool.save_result_json(
    final_output=final_output,      # ì €ì¥í•  ë°ì´í„° (dict, list, ë˜ëŠ” JSON ë¬¸ìì—´)
    output_filename="quiz_result",  # íŒŒì¼ëª… (í™•ì¥ì ìë™ ì¶”ê°€)
    save_foldername="final_outputs" # ì €ì¥í•  í´ë”ëª… (ìë™ ìƒì„±)
)

print("âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: final_outputs/quiz_result.json")
```

## 3. LLM ì—ì´ì „íŠ¸ ìƒì„¸ ì‚¬ìš©ë²•

### ê¸°ë³¸ ì—ì´ì „íŠ¸ ìƒì„±í•˜ê¸°

#### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

```python
# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from module.llm_agent import LLM_Agent

# LLM ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
agent = LLM_Agent(
    model_name="gemini-2.5-flash",  # ì‚¬ìš©í•  ëª¨ë¸ëª… (ì˜ˆ: "gemma:12b", "gpt-4", "gemini-2.5-flash")
    provider="genai",               # LLM ì œê³µì ("ollama", "openai", "genai")
    api_key="your_api_key",         # API í‚¤ (ollamaëŠ” None, ë‚˜ë¨¸ì§€ëŠ” í•„ìˆ˜)
    session_id="chat_session",      # ë©”ëª¨ë¦¬ ê¸°ëŠ¥ìš© ì„¸ì…˜ ID (ì„ íƒì‚¬í•­)
    max_history=10                  # ê¸°ì–µí•  ëŒ€í™” ìˆ˜ (ì„ íƒì‚¬í•­, ê¸°ë³¸: 10)
)
```

#### ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ

```python
model_name = 'gemma:12b' # ì‚¬ìš©í•  ëª¨ë¸ëª…ì„ ì…ë ¥í•˜ì„¸ìš”
system_prompt = 'ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ ë¹„ì„œì…ë‹ˆë‹¤. ì´ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.'
user_prompt = 'í”„ë‘ìŠ¤ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?'
provider = 'ollama'  # í˜„ì¬ ì‚¬ìš©ê°€ëŠ¥í•œ providerëŠ” "ollama", "openai","genai(gemini)"ì…ë‹ˆë‹¤

agent = LLM_Agent(model_name, provider, api_key=None)

response = agent(system_prompt, user_prompt, task=None)
print(response)
```

**ì„¤ëª…:**
- `model_name`: ì‚¬ìš©í•  ëª¨ë¸ì˜ ì´ë¦„ì„ ì§€ì •í•©ë‹ˆë‹¤
- `system_prompt`: ëª¨ë¸ì—ê²Œ ì£¼ì–´ì§€ëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€ì´ë¯€ë¡œ ì—­í•  ë° í˜ë¥´ì†Œë‚˜ë¥¼ ì§€ì •í•˜ì„¸ìš”
- `user_prompt`: LLMì—ê²Œ ì§ˆë¬¸ ë° ì‘ì—…ì„ ìš”ì²­í•˜ì„¸ìš”
- `provider`: ì‚¬ìš©í•  LLM ì œê³µìë¥¼ ì§€ì •í•©ë‹ˆë‹¤
- `task`ì˜ ê¸°ë³¸ê°’ì€ Noneì´ê³  í•„ìš”ì‹œ ì¶”ê°€í•˜ì„¸ìš”
- `api_key`ì˜ ê¸°ë³¸ê°’ì€ Noneì´ë¯€ë¡œ, ollamaë¥¼ ì‚¬ìš©í•  ê²½ìš° API í‚¤ê°€ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤

### ë©€í‹° ì—ì´ì „íŠ¸ í™œìš©í•˜ê¸°

#### ê¸°ë³¸ ë©€í‹° ì—ì´ì „íŠ¸ ì„¤ì •

```python
# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from module.llm_agent import LLM_Agent

# ë©€í‹° ì—ì´ì „íŠ¸ìš© LLM ì—ì´ì „íŠ¸ ìƒì„±
multi_agent = LLM_Agent(
    model_name="gemma3n",           # ì‚¬ìš©í•  ëª¨ë¸ëª…
    provider="ollama"               # LLM ì œê³µì (ollama/openai/genai)
)

# ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ ì‹¤í–‰
agent1_response = multi_agent(
    system_prompt="You are a helpful assistant.",
    user_message="ë¦¬ëˆ…ìŠ¤ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    task=None                       # ì¶”ê°€ ì‘ì—… ë‚´ìš© (ì„ íƒì‚¬í•­)
)

# ë‘ ë²ˆì§¸ ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì²« ë²ˆì§¸ ì‘ë‹µ í™œìš©)
agent2_response = multi_agent(
    system_prompt="You are a helpful assistant.",
    user_message="ì•ì„  ë‹µë³€ì„ ì½ê³  ë‚´ìš©ì„ ë³´ì¶©í•´ ì£¼ì„¸ìš”",
    task=None,
    multi_agent_response=agent1_response  # ì´ì „ ì—ì´ì „íŠ¸ ì‘ë‹µì„ ì „ë‹¬
)
```

#### ìˆœì°¨ì  ë©€í‹° ì—ì´ì „íŠ¸ ì˜ˆì œ

```python
from module.llm_agent import LLM_Agent

system_prompt = "You are a helpful assistant."
agent1_user_prompt = "ë¦¬ëˆ…ìŠ¤ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
agent2_user_prompt = "ì•ì„  ë‹µë³€ì„ ì½ê³  ë‚´ìš©ì„ ë³´ì¶©í•´ ì£¼ì„¸ìš”"

# ë³µìˆ˜ì˜ ì—ì´ì „íŠ¸ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
multi_agent = LLM_Agent(model_name="gemma3n", provider="ollama")
agent1 = multi_agent(system_prompt, agent1_user_prompt, task=None)
agent2 = multi_agent(system_prompt, agent2_user_prompt, task=None, multi_agent_response=agent1)

# agent1ì˜ ë‹µë³€ì„ ì´ì–´ ë°›ì•„, agent2ì˜ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.
print("Agent 1 Response:", agent1)
print("Agent 2 Response:", agent2)
```

**ë©€í‹° ì—ì´ì „íŠ¸ í™œìš© ë°©ë²•:**
- ì—¬ëŸ¬ê°œì˜ LLM ì—ì´ì „íŠ¸ë¥¼ êµ¬ì„±í•˜ì„¸ìš”
- ìˆœì°¨ì ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•˜ê³ , ì•ì˜ ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ë‹¤ìŒ ì—ì´ì „íŠ¸ì˜ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤
- `multi_agent_response` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ë‹¤ìŒ ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤
- ì‘ì—…ì„ ë¶„í• í•˜ê³  ê° ì—ì´ì „íŠ¸ì— ë§ê²Œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

### ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ì‘ë‹µ í†µí•©í•˜ê¸°

#### ê³ ê¸‰ ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ

```python
# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from module.llm_agent import LLM_Agent

# ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ìƒì„±
multi_agent = LLM_Agent(
    model_name="gemini-2.5-flash",  # ì‚¬ìš©í•  ëª¨ë¸ëª…
    provider="genai",               # LLM ì œê³µì
    api_key="your_api_key"          # API í‚¤
)

# ê° ì—ì´ì „íŠ¸ë³„ ì—­í• ê³¼ ì‘ì—… ì •ì˜
multi_agent_tasks = {
    "Agent 1": "í™˜ê²½ ë¬¸ì œ ë¶„ì„ ì‘ì—…",
    "Agent 2": "êµí†µ ì¸í”„ë¼ ê³„íš ì‘ì—…",
    "Agent 3": "ì—ë„ˆì§€ ê³µê¸‰ ë°©ì•ˆ ì„¤ê³„",
    "Agent 4": "ë„ì‹œ ê³µê°„ êµ¬ì¡° ìµœì í™”"
}

# ê° ì—ì´ì „íŠ¸ ì‹¤í–‰ í›„ ì‘ë‹µ ìˆ˜ì§‘
agent_responses = {
    name: multi_agent(system_prompt, user_prompt, task)
    for name, task in multi_agent_tasks.items()
}

# ìµœì¢… í†µí•© ì—ì´ì „íŠ¸ ì‹¤í–‰
final_response = multi_agent(
    "í†µí•© ì „ë¬¸ê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
    "ëª¨ë“  ì „ë¬¸ê°€ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ë¡ ì„ ë‚´ë ¤ì£¼ì„¸ìš”",
    "ìµœì¢… í†µí•© ì‘ì—…",
    list(agent_responses.values())  # ëª¨ë“  ì—ì´ì „íŠ¸ ì‘ë‹µì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬
)
```

#### ì‹¤ì œ êµ¬í˜„ ì˜ˆì œ (ë„ì‹œ ê³„íš)

```python
from module.llm_agent import LLM_Agent

# ê° ì—ì´ì „íŠ¸ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸, ì‘ì—…ì„ ì •ì˜í•©ë‹ˆë‹¤.
multi_agent_tasks = {
    "Agent 1": "ë„ì‹œì—ì„œ ë°œìƒí•˜ëŠ” í™˜ê²½ ë¬¸ì œ(ëŒ€ê¸°, ìˆ˜ì§ˆ, ì“°ë ˆê¸° ë“±)ë¥¼ ì •ë¦¬í•˜ê³ , ê°€ì¥ ì‹œê¸‰í•œ ê³¼ì œë¥¼ ì œì‹œí•œë‹¤.",
    "Agent 2": "ì¹œí™˜ê²½ êµí†µìˆ˜ë‹¨(ëŒ€ì¤‘êµí†µ, ìì „ê±°, ì „ê¸°ì°¨ ë“±)ì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì† ê°€ëŠ¥í•œ êµí†µ ì¸í”„ë¼ ê³„íšì„ ì œì•ˆí•œë‹¤.",
    "Agent 3": "ì¬ìƒì—ë„ˆì§€(íƒœì–‘ê´‘, í’ë ¥, ìŠ¤ë§ˆíŠ¸ ê·¸ë¦¬ë“œ ë“±)ë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ì—ë„ˆì§€ ê³µê¸‰ ë°©ì•ˆì„ ì„¤ê³„í•œë‹¤.",
    "Agent 4": "ë„ì‹œ ê³µê°„ êµ¬ì¡°(ê³µì›, ì£¼ê±°, ìƒì—…ì§€êµ¬ ë°°ì¹˜ ë“±)ë¥¼ ìµœì í™”í•œë‹¤."
}

multi_agent_system_prompts = {
    "Agent 1": "ë‹¹ì‹ ì€ í™˜ê²½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë„ì‹œì˜ í™˜ê²½ ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³ , ê°€ì¥ ì‹œê¸‰í•œ ë¬¸ì œë¥¼ ì œì‹œí•˜ì„¸ìš”.",
    "Agent 2": "ë‹¹ì‹ ì€ êµí†µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§€ì† ê°€ëŠ¥í•œ êµí†µ ì¸í”„ë¼ ê³„íšì„ ì œì•ˆí•˜ì„¸ìš”.",
    "Agent 3": "ë‹¹ì‹ ì€ ì—ë„ˆì§€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¬ìƒì—ë„ˆì§€ë¥¼ í™œìš©í•œ ì—ë„ˆì§€ ê³µê¸‰ ë°©ì•ˆì„ ì„¤ê³„í•˜ì„¸ìš”.",
    "Agent 4": "ë‹¹ì‹ ì€ ë„ì‹œ ê³„íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë„ì‹œ ê³µê°„ êµ¬ì¡°ë¥¼ ìµœì í™”í•˜ëŠ” ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”."
}

user_prompts = {
    "Agent 1": "ë„ì‹œì—ì„œ ë°œìƒí•˜ëŠ” í™˜ê²½ ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³ , ê°€ì¥ ì‹œê¸‰í•œ ë¬¸ì œë¥¼ ì œì‹œí•˜ì„¸ìš”.",
    "Agent 2": "ì§€ì† ê°€ëŠ¥í•œ êµí†µ ì¸í”„ë¼ ê³„íšì„ ì œì•ˆí•˜ì„¸ìš”.",
    "Agent 3": "ì¬ìƒì—ë„ˆì§€ë¥¼ í™œìš©í•œ ì—ë„ˆì§€ ê³µê¸‰ ë°©ì•ˆì„ ì„¤ê³„í•˜ì„¸ìš”.",
    "Agent 4": "ë„ì‹œ ê³µê°„ êµ¬ì¡°ë¥¼ ìµœì í™”í•˜ëŠ” ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”."
}

order = ["Agent 1", "Agent 2", "Agent 3", "Agent 4"]

multi_agent = LLM_Agent(model_name="gemini-2.5-flash", provider="genai", api_key="your_api_key")

agent_responses = {
    name: multi_agent(multi_agent_system_prompts[name], user_prompts[name], multi_agent_tasks[name])
    for name in order
}
response_list = [agent_responses[name] for name in order]  # ê° ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ ìˆœì„œëŒ€ë¡œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.

multi_agent_responses = multi_agent(
    "ë‹¹ì‹ ì€ ë„ì‹œ ê³„íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§€ì† ê°€ëŠ¥í•œ ë„ì‹œ ì„¤ê³„ ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.",
    "ë‹¤ìŒì€ ì—¬ëŸ¬ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ìš”ì•½ ë° í†µí•©ëœ ì§€ì† ê°€ëŠ¥í•œ ë„ì‹œ ì„¤ê³„ ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.",
    "ìµœì¢… ìš”ì•½ ë° í†µí•©ëœ ì§€ì† ê°€ëŠ¥í•œ ë„ì‹œ ì„¤ê³„ ë°©ì•ˆì„ ì œì‹œí•œë‹¤.",
    response_list
)  # ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ì‘ë‹µì„ í†µí•©í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

print("Agent 1 Response:", agent_responses["Agent 1"])
print("Agent 2 Response:", agent_responses["Agent 2"])
print("Agent 3 Response:", agent_responses["Agent 3"])
print("Agent 4 Response:", agent_responses["Agent 4"])
print("Multi-Agent Responses:", multi_agent_responses)
```

**ì—ì´ì „íŠ¸ í†µí•© ë°©ë²•:**
- ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ê°ê° í• ë‹¹í•˜ê³  í†µí•©í•˜ì—¬ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤
- ê° ì—ì´ì „íŠ¸ëŠ” íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³ , ê·¸ ì‘ë‹µì€ ìµœì¢… í†µí•© ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤
- ì—ì´ì „íŠ¸ ë³„ë¡œ ê°ê°ì˜ ì‘ì—…ì„ í• ë‹¹í•˜ê³ , í•˜ë‚˜ë¡œ í†µí•©í•œ ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤

### LLM ì—ì´ì „íŠ¸ì— ê¸°ì–µë ¥ ë¶™ì´ê¸°

#### ë©”ëª¨ë¦¬ ê¸°ëŠ¥ ì„¤ì •

```python
# í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
import dotenv, os
from module.llm_agent import LLM_Agent

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
dotenv.load_dotenv()

# ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì´ ìˆëŠ” LLM ì—ì´ì „íŠ¸ ìƒì„±
llm = LLM_Agent(
    model_name="gemini-2.5-flash",     # ì‚¬ìš©í•  ëª¨ë¸ëª…
    provider="genai",                  # LLM ì œê³µì
    api_key=os.getenv("GENAI_API_KEY"), # API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
    session_id="user_chat_session",    # ëŒ€í™” ì„¸ì…˜ êµ¬ë¶„ ID (ì„ íƒì‚¬í•­)
    max_history=10                     # ê¸°ì–µí•  ëŒ€í™” ìˆ˜ (ì„ íƒì‚¬í•­, ê¸°ë³¸: 10)
)

# ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í™œì„±í™”í•˜ì—¬ ëŒ€í™”í•˜ê¸°
response = llm(
    system_prompt="You are a helpful assistant.",
    user_message="ì•ˆë…•í•˜ì„¸ìš”, ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤.",
    memory=True                        # ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í™œì„±í™” (ì¤‘ìš”!)
)

# ë‹¤ìŒ ëŒ€í™”ì—ì„œ ì´ì „ ë‚´ìš© ê¸°ì–µë¨
next_response = llm(
    system_prompt="You are a helpful assistant.",
    user_message="ì œ ì´ë¦„ì´ ë­ë¼ê³  í–ˆì£ ?",
    memory=True                        # ì´ì „ ëŒ€í™” ê¸°ì–µí•˜ì—¬ ì‘ë‹µ
)
```

#### ì—°ì† ëŒ€í™” ì˜ˆì œ

```python
import dotenv
import os
dotenv.load_dotenv()

# LLM_Agent ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = LLM_Agent(
    model_name="gemini-2.5-flash",
    provider="genai",
    api_key=os.getenv("GENAI_API_KEY"),
    max_history=10  # max_history = ê¸°ì–µí•  ëŒ€í™” ìˆ˜
)

# ëŒ€í™” ë£¨í”„ ì˜ˆì‹œ
while True:
    user_input = input("You: ")
    response = llm(
        system_prompt="You are a helpful assistant.",
        user_message=user_input,
        memory=True  # memory=Trueë¡œ ì„¤ì •í•˜ì—¬ ê¸°ì–µë ¥ í™œì„±í™”
    )
    print("Assistant:", response)

    if user_input.lower() in ['exit', 'quit']:
        break
```

**ë©”ëª¨ë¦¬ ê¸°ëŠ¥ í™œìš© ë°©ë²•:**
- LLM ì—ì´ì „íŠ¸ì— ê¸°ì–µë ¥ì„ ì¶”ê°€í•˜ì—¬ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- `max_history=10`ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ 10ê°œì˜ ì´ì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤
- ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì€ SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ì—¬ ì—°ì†ì ì¸ ëŒ€í™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤

---

ì´ ë¬¸ì„œëŠ” AI Multi-Agent Toolkitì˜ ì£¼ìš” ê¸°ëŠ¥ê³¼ ì‚¬ìš©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤. ë” ìì„¸í•œ ì •ë³´ëŠ” ê° ëª¨ë“ˆì˜ ì†ŒìŠ¤ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.