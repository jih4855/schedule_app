import os
import sys
import logging
from typing import List, Dict, Any, Optional

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# í”„ë¡œì íŠ¸ì˜ ì‹¤ì œ ëª¨ë“ˆë“¤ì„ ì„í¬íŠ¸
from module.audio_tool import Audio
from module.llm_agent import LLM_Agent, Multi_modal_agent
from module.discord import Send_to_discord

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="AI Multi-Agent Toolkit API",
    description="ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ì˜ ë©€í‹°ëª¨ë‹¬ AI ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œìŠ¤í…œ",
    version="2.0.0"
)

# CORS ì„¤ì •
origins = [
    "http://localhost:3000",
    "http://localhost:3001",
    "http://localhost:3002",
    "http://127.0.0.1:3000",
    "http://127.0.0.1:3001",
    "http://127.0.0.1:3002",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === Pydantic ëª¨ë¸ ì •ì˜ ===

class WorkflowStep(BaseModel):
    step: int = Field(..., description="ì‹¤í–‰ ìˆœì„œ")
    module: str = Field(..., description="ì‹¤í–‰í•  ëª¨ë“ˆ ì´ë¦„ (Audio, LLM_Agent, Multi_modal_agent, Discord)")
    action: str = Field(..., description="ì‹¤í–‰í•  ë©”ì„œë“œ ì´ë¦„")
    params: Dict[str, Any] = Field(default_factory=dict, description="ë©”ì„œë“œì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°")
    init_params: Dict[str, Any] = Field(default_factory=dict, description="ëª¨ë“ˆ ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°")

class WorkflowRequest(BaseModel):
    workflow: List[WorkflowStep] = Field(..., description="ì‹¤í–‰í•  ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ë“¤")

class WorkflowResponse(BaseModel):
    status: str
    step_outputs: Dict[int, Any] = {}
    final_result: Any = None
    error_message: Optional[str] = None

# === ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í´ë˜ìŠ¤ ===

class WorkflowOrchestrator:
    def __init__(self, workflow_steps: List[WorkflowStep]):
        self.steps = sorted(workflow_steps, key=lambda x: x.step)
        self.step_outputs = {}

        # ğŸ”¥ í•µì‹¬! ë¬¸ìì—´ ëª¨ë“ˆëª…ì„ ì‹¤ì œ í´ë˜ìŠ¤ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        self.module_map = {
            "Audio": Audio,
            "LLM_Agent": LLM_Agent,
            "Multi_modal_agent": Multi_modal_agent,
            "Discord": Send_to_discord,
        }

    def execute(self) -> Dict:
        """ì›Œí¬í”Œë¡œìš°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜"""
        try:
            for step in self.steps:
                logger.info(f"--- ë‹¨ê³„ {step.step} ì‹¤í–‰: {step.module}.{step.action} ---")

                # ì´ì „ ë‹¨ê³„ ì¶œë ¥ì„ ì°¸ì¡°í•˜ëŠ” íŒŒë¼ë¯¸í„°ë“¤ì„ ì‹¤ì œ ê°’ìœ¼ë¡œ êµì²´
                resolved_params = self._resolve_params(step.params)

                # 1. ëª¨ë“ˆ í´ë˜ìŠ¤ ì°¾ê¸°
                module_class = self.module_map.get(step.module)
                if not module_class:
                    raise ValueError(f"ëª¨ë“ˆ '{step.module}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ: {list(self.module_map.keys())}")

                # 2. ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì´ˆê¸°í™” íŒŒë¼ë¯¸í„° í¬í•¨)
                try:
                    if step.init_params:
                        module_instance = module_class(**step.init_params)
                    else:
                        module_instance = module_class()
                except TypeError as e:
                    logger.error(f"ëª¨ë“ˆ {step.module} ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                    # íŒŒë¼ë¯¸í„° ì—†ì´ ì¬ì‹œë„
                    module_instance = module_class()

                # 3. ë©”ì„œë“œ ì°¾ê¸° ë° ì‹¤í–‰
                action_method = getattr(module_instance, step.action, None)
                if not action_method:
                    available_methods = [method for method in dir(module_instance)
                                       if not method.startswith('_') and callable(getattr(module_instance, method))]
                    raise ValueError(f"ì•¡ì…˜ '{step.action}'ì„ ëª¨ë“ˆ '{step.module}'ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                                   f"ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œ: {available_methods}")

                # 4. ë©”ì„œë“œ ì‹¤í–‰
                if resolved_params:
                    output = action_method(**resolved_params)
                else:
                    output = action_method()

                # 5. ê²°ê³¼ ì €ì¥
                self.step_outputs[step.step] = output
                logger.info(f"ë‹¨ê³„ {step.step} ì™„ë£Œ. ì¶œë ¥: {str(output)[:100]}...")

            return {
                "status": "success",
                "step_outputs": self.step_outputs,
                "final_result": self.step_outputs.get(len(self.steps)) if self.steps else None
            }

        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "status": "error",
                "error_message": str(e),
                "step_outputs": self.step_outputs
            }

    def _resolve_params(self, params: Dict) -> Dict:
        """íŒŒë¼ë¯¸í„°ì—ì„œ ì´ì „ ë‹¨ê³„ ì°¸ì¡°ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ êµì²´"""
        resolved = {}
        for key, value in params.items():
            if isinstance(value, str) and value.startswith("step_output_"):
                try:
                    # "step_output_1" -> 1
                    source_step_num = int(value.split('_')[-1])
                    if source_step_num in self.step_outputs:
                        resolved[key] = self.step_outputs[source_step_num]
                    else:
                        raise ValueError(f"ë‹¨ê³„ {source_step_num}ì˜ ì¶œë ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except (ValueError, IndexError):
                    logger.warning(f"ì˜ëª»ëœ ì°¸ì¡° í˜•ì‹: {value}. ì›ë³¸ ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    resolved[key] = value
            else:
                resolved[key] = value
        return resolved

# === API ì—”ë“œí¬ì¸íŠ¸ ===

@app.get("/")
def read_root():
    return {
        "message": "ğŸš€ AI Multi-Agent Toolkit - ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ APIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!",
        "version": "2.0.0",
        "features": [
            "ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ AI íŒŒì´í”„ë¼ì¸",
            "ë™ì  ëª¨ë“ˆ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜",
            "ë‹¨ê³„ë³„ ê²°ê³¼ ì—°ê²°",
            "í™•ì¥ ê°€ëŠ¥í•œ ëª¨ë“ˆ ì•„í‚¤í…ì²˜"
        ],
        "endpoints": {
            "/execute_workflow": "ì›Œí¬í”Œë¡œìš° ì‹¤í–‰",
            "/docs": "API ë¬¸ì„œ",
            "/modules": "ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆ ëª©ë¡"
        }
    }

@app.get("/modules")
def get_available_modules():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆê³¼ ë©”ì„œë“œ ëª©ë¡ì„ ë°˜í™˜"""
    modules_info = {}

    # ì„ì‹œ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ì–´ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ì„œë“œ ì¶”ì¶œ
    module_map = {
        "Audio": Audio,
        "LLM_Agent": LLM_Agent,
        "Multi_modal_agent": Multi_modal_agent,
        "Discord": Send_to_discord,
    }

    for module_name, module_class in module_map.items():
        try:
            instance = module_class()
            methods = [method for method in dir(instance)
                      if not method.startswith('_') and callable(getattr(instance, method))]
            modules_info[module_name] = {
                "description": instance.__class__.__doc__ or f"{module_name} ëª¨ë“ˆ",
                "methods": methods
            }
        except Exception as e:
            modules_info[module_name] = {
                "description": f"{module_name} ëª¨ë“ˆ (ì´ˆê¸°í™” íŒŒë¼ë¯¸í„° í•„ìš”í•  ìˆ˜ ìˆìŒ)",
                "error": str(e)
            }

    return {"available_modules": modules_info}

@app.get("/examples")
def get_workflow_examples():
    """ì›Œí¬í”Œë¡œìš° ì‚¬ìš© ì˜ˆì œë“¤ì„ ë°˜í™˜"""
    return {
        "workflow_examples": {
            "simple_llm": {
                "name": "ê¸°ë³¸ LLM í˜¸ì¶œ",
                "description": "ë‹¨ì¼ LLM ì—ì´ì „íŠ¸ í˜¸ì¶œ ì˜ˆì œ",
                "workflow": {
                    "workflow": [
                        {
                            "step": 1,
                            "module": "LLM_Agent",
                            "action": "__call__",
                            "init_params": {"model_name": "gemma3n", "provider": "ollama"},
                            "params": {
                                "system_prompt": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                                "user_message": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                            }
                        }
                    ]
                }
            },
            "multi_agent_discussion": {
                "name": "ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡ ",
                "description": "ì—¬ëŸ¬ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ê°€ í† ë¡ í•˜ê³  ìµœì¢… ê²°ë¡ ì„ ë„ì¶œ",
                "workflow": {
                    "workflow": [
                        {
                            "step": 1,
                            "module": "LLM_Agent",
                            "action": "__call__",
                            "init_params": {"model_name": "gemma3n", "provider": "ollama"},
                            "params": {
                                "system_prompt": "ë‹¹ì‹ ì€ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                                "user_message": "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                                "task": "ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ê´€ì ì—ì„œ ì„¤ëª…í•˜ì„¸ìš”."
                            }
                        },
                        {
                            "step": 2,
                            "module": "LLM_Agent",
                            "action": "__call__",
                            "init_params": {"model_name": "gemma3n", "provider": "ollama"},
                            "params": {
                                "system_prompt": "ë‹¹ì‹ ì€ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.",
                                "user_message": "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                                "task": "êµ¬í˜„ ê´€ì ì—ì„œ ì„¤ëª…í•˜ì„¸ìš”."
                            }
                        },
                        {
                            "step": 3,
                            "module": "LLM_Agent",
                            "action": "aggregate_responses",
                            "init_params": {"model_name": "gemma3n", "provider": "ollama"},
                            "params": {
                                "system_prompt": "ë‹¹ì‹ ì€ ì¢…í•© ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                                "user_message": "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                                "task": "ì „ë¬¸ê°€ë“¤ì˜ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
                                "responses": ["step_output_1", "step_output_2"]
                            }
                        }
                    ]
                }
            },
            "audio_to_text_analysis": {
                "name": "ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸ â†’ ë¶„ì„ íŒŒì´í”„ë¼ì¸",
                "description": "ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  LLMìœ¼ë¡œ ë¶„ì„",
                "workflow": {
                    "workflow": [
                        {
                            "step": 1,
                            "module": "Audio",
                            "action": "transcribe_audio",
                            "init_params": {"text_output": "output", "source_file": "audio_files"},
                            "params": {
                                "whisper_model": "base"
                            }
                        },
                        {
                            "step": 2,
                            "module": "LLM_Agent",
                            "action": "__call__",
                            "init_params": {"model_name": "gemma3n", "provider": "ollama"},
                            "params": {
                                "system_prompt": "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                                "user_message": "step_output_1",
                                "task": "ì´ í…ìŠ¤íŠ¸ì˜ ì£¼ìš” ë‚´ìš©ê³¼ ê°ì •ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
                            }
                        }
                    ]
                }
            },
            "analysis_to_discord": {
                "name": "ë¶„ì„ ê²°ê³¼ë¥¼ Discordë¡œ ì „ì†¡",
                "description": "LLM ë¶„ì„ ê²°ê³¼ë¥¼ Discord ì±„ë„ë¡œ ì „ì†¡",
                "workflow": {
                    "workflow": [
                        {
                            "step": 1,
                            "module": "LLM_Agent",
                            "action": "__call__",
                            "init_params": {"model_name": "gemma3n", "provider": "ollama"},
                            "params": {
                                "system_prompt": "ë‹¹ì‹ ì€ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                                "user_message": "ì˜¤ëŠ˜ì˜ AI ê¸°ìˆ  ë™í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
                            }
                        },
                        {
                            "step": 2,
                            "module": "Discord",
                            "action": "send_message",
                            "init_params": {"base_url": "YOUR_DISCORD_WEBHOOK_URL"},
                            "params": {
                                "message": "step_output_1"
                            }
                        }
                    ]
                }
            },
            "multimodal_workflow": {
                "name": "ë³µí•© ë©€í‹°ëª¨ë‹¬ ì›Œí¬í”Œë¡œìš°",
                "description": "ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸ â†’ ë©€í‹°ì—ì´ì „íŠ¸ ë¶„ì„ â†’ Discord ì „ì†¡",
                "workflow": {
                    "workflow": [
                        {
                            "step": 1,
                            "module": "Audio",
                            "action": "transcribe_audio",
                            "init_params": {"text_output": "transcripts", "source_file": "meeting_audio"},
                            "params": {"whisper_model": "large-v3"}
                        },
                        {
                            "step": 2,
                            "module": "LLM_Agent",
                            "action": "__call__",
                            "init_params": {"model_name": "gemma3n", "provider": "ollama"},
                            "params": {
                                "system_prompt": "ë‹¹ì‹ ì€ íšŒì˜ë¡ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                                "user_message": "step_output_1",
                                "task": "ì´ íšŒì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  ì£¼ìš” ê²°ì •ì‚¬í•­ì„ ì¶”ì¶œí•˜ì„¸ìš”."
                            }
                        },
                        {
                            "step": 3,
                            "module": "LLM_Agent",
                            "action": "__call__",
                            "init_params": {"model_name": "gemma3n", "provider": "ollama"},
                            "params": {
                                "system_prompt": "ë‹¹ì‹ ì€ ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                                "user_message": "step_output_1",
                                "task": "ë‹¤ìŒ ë‹¨ê³„ë¡œ í•´ì•¼ í•  ì¼ë“¤ì„ ì •ë¦¬í•˜ì„¸ìš”."
                            }
                        },
                        {
                            "step": 4,
                            "module": "Discord",
                            "action": "send_message",
                            "init_params": {"base_url": "YOUR_DISCORD_WEBHOOK_URL"},
                            "params": {
                                "message": "ğŸ“ íšŒì˜ ìš”ì•½:\n\nstep_output_2\n\nâœ… ì•¡ì…˜ ì•„ì´í…œ:\n\nstep_output_3"
                            }
                        }
                    ]
                }
            }
        },
        "usage_tips": [
            "ì›Œí¬í”Œë¡œìš°ëŠ” step ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.",
            "step_output_Nì„ ì‚¬ìš©í•´ ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "init_paramsë¡œ ê° ëª¨ë“ˆì˜ ì´ˆê¸°í™” ì„¤ì •ì„ ì§€ì •í•©ë‹ˆë‹¤.",
            "paramsë¡œ ë©”ì„œë“œì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.",
            "/modules ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“ˆê³¼ ë©”ì„œë“œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        ]
    }

@app.get("/workflow-builder")
def get_workflow_builder_config():
    """ì›¹ ì¸í„°í˜ì´ìŠ¤ìš© ì›Œí¬í”Œë¡œìš° ë¹Œë” ì„¤ì •ì„ ë°˜í™˜"""
    return {
        "modules": {
            "Audio": {
                "name": "ì˜¤ë””ì˜¤ ì²˜ë¦¬",
                "description": "ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜",
                "init_params": {
                    "text_output": {
                        "type": "string",
                        "description": "í…ìŠ¤íŠ¸ ì¶œë ¥ í´ë”ëª…",
                        "default": "transcripts",
                        "required": False
                    },
                    "source_file": {
                        "type": "string",
                        "description": "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìˆëŠ” í´ë”ëª…",
                        "default": "audio_files",
                        "required": False
                    }
                },
                "actions": {
                    "transcribe_audio": {
                        "name": "ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜",
                        "params": {
                            "whisper_model": {
                                "type": "select",
                                "options": ["tiny", "base", "small", "medium", "large", "large-v2", "large-v3"],
                                "default": "base",
                                "description": "Whisper ëª¨ë¸ í¬ê¸° (í´ìˆ˜ë¡ ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)"
                            },
                            "audio_extensions": {
                                "type": "multiselect",
                                "options": ["mp3", "wav", "m4a", "flac", "aac", "ogg"],
                                "default": ["mp3", "wav", "m4a"],
                                "description": "ì²˜ë¦¬í•  ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¥ì"
                            }
                        }
                    }
                }
            },
            "LLM_Agent": {
                "name": "LLM ì—ì´ì „íŠ¸",
                "description": "ë‹¤ì–‘í•œ LLM ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±",
                "init_params": {
                    "model_name": {
                        "type": "string",
                        "description": "ì‚¬ìš©í•  ëª¨ë¸ëª…",
                        "default": "gemma3n",
                        "required": True
                    },
                    "provider": {
                        "type": "select",
                        "options": ["ollama", "openai", "genai"],
                        "default": "ollama",
                        "description": "LLM ì œê³µì—…ì²´"
                    },
                    "api_key": {
                        "type": "password",
                        "description": "API í‚¤ (OpenAI, Google AI ì‚¬ìš©ì‹œ í•„ìš”)",
                        "required": False
                    },
                    "session_id": {
                        "type": "string",
                        "description": "ì„¸ì…˜ ID (ë©”ëª¨ë¦¬ ê¸°ëŠ¥ìš©)",
                        "default": "default_session",
                        "required": False
                    }
                },
                "actions": {
                    "__call__": {
                        "name": "í…ìŠ¤íŠ¸ ìƒì„±",
                        "params": {
                            "system_prompt": {
                                "type": "textarea",
                                "description": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (AIì˜ ì—­í•  ì •ì˜)",
                                "required": True,
                                "placeholder": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
                            },
                            "user_message": {
                                "type": "textarea",
                                "description": "ì‚¬ìš©ì ë©”ì‹œì§€ ë˜ëŠ” ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì°¸ì¡°",
                                "required": True,
                                "placeholder": "ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜ step_output_1 ê°™ì€ ì°¸ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"
                            },
                            "task": {
                                "type": "textarea",
                                "description": "êµ¬ì²´ì ì¸ ì‘ì—… ì„¤ëª… (ì„ íƒì‚¬í•­)",
                                "required": False,
                                "placeholder": "ì˜ˆ: ì´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”"
                            },
                            "memory": {
                                "type": "checkbox",
                                "description": "ëŒ€í™” ê¸°ë¡ ì €ì¥",
                                "default": False
                            }
                        }
                    },
                    "aggregate_responses": {
                        "name": "ì‘ë‹µ ì¢…í•©",
                        "params": {
                            "system_prompt": {
                                "type": "textarea",
                                "description": "ì¢…í•© ë¶„ì„ì„ ìœ„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                                "required": True,
                                "placeholder": "ë‹¹ì‹ ì€ ì¢…í•© ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                            },
                            "user_message": {
                                "type": "textarea",
                                "description": "ì›ë˜ ì§ˆë¬¸",
                                "required": True
                            },
                            "task": {
                                "type": "textarea",
                                "description": "ì¢…í•© ë¶„ì„ ì‘ì—…",
                                "required": True,
                                "placeholder": "ì „ë¬¸ê°€ë“¤ì˜ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”"
                            },
                            "responses": {
                                "type": "array",
                                "description": "ì¢…í•©í•  ì´ì „ ë‹¨ê³„ë“¤ (ì˜ˆ: [\"step_output_1\", \"step_output_2\"])",
                                "required": True,
                                "placeholder": "[\"step_output_1\", \"step_output_2\"]"
                            }
                        }
                    }
                }
            },
            "Multi_modal_agent": {
                "name": "ë©€í‹°ëª¨ë‹¬ ì—ì´ì „íŠ¸",
                "description": "ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬",
                "init_params": {
                    "model_name": {
                        "type": "string",
                        "description": "ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ëª…",
                        "default": "llava",
                        "required": True
                    },
                    "provider": {
                        "type": "select",
                        "options": ["ollama", "openai"],
                        "default": "ollama",
                        "description": "ì œê³µì—…ì²´"
                    }
                },
                "actions": {
                    "__call__": {
                        "name": "ë©€í‹°ëª¨ë‹¬ ë¶„ì„",
                        "params": {
                            "system_prompt": {
                                "type": "textarea",
                                "description": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
                                "required": True
                            },
                            "user_message": {
                                "type": "textarea",
                                "description": "ì‚¬ìš©ì ë©”ì‹œì§€",
                                "required": True
                            },
                            "image_path": {
                                "type": "string",
                                "description": "ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ",
                                "required": False
                            }
                        }
                    }
                }
            },
            "Discord": {
                "name": "Discord ì „ì†¡",
                "description": "Discord ì±„ë„ë¡œ ë©”ì‹œì§€ ì „ì†¡",
                "init_params": {
                    "base_url": {
                        "type": "url",
                        "description": "Discord ì›¹í›… URL",
                        "required": True,
                        "placeholder": "https://discord.com/api/webhooks/..."
                    },
                    "chunk_size": {
                        "type": "number",
                        "description": "ë©”ì‹œì§€ ë¶„í•  í¬ê¸°",
                        "default": 1900,
                        "min": 100,
                        "max": 2000
                    },
                    "overlap": {
                        "type": "number",
                        "description": "ë¶„í• ì‹œ ê²¹ì¹˜ëŠ” ê¸€ì ìˆ˜",
                        "default": 0,
                        "min": 0,
                        "max": 200
                    }
                },
                "actions": {
                    "send_message": {
                        "name": "ë©”ì‹œì§€ ì „ì†¡",
                        "params": {
                            "message": {
                                "type": "textarea",
                                "description": "ì „ì†¡í•  ë©”ì‹œì§€ ë˜ëŠ” ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì°¸ì¡°",
                                "required": True,
                                "placeholder": "ë©”ì‹œì§€ ë‚´ìš© ë˜ëŠ” step_output_1"
                            }
                        }
                    }
                }
            }
        },
        "workflow_templates": [
            {
                "name": "ê°„ë‹¨í•œ ì§ˆë¬¸ë‹µë³€",
                "description": "ê¸°ë³¸ì ì¸ LLM ì§ˆë¬¸ë‹µë³€",
                "steps": 1,
                "template": [
                    {
                        "module": "LLM_Agent",
                        "action": "__call__"
                    }
                ]
            },
            {
                "name": "ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡ ",
                "description": "ì—¬ëŸ¬ ì „ë¬¸ê°€ê°€ í† ë¡ í•˜ê³  ê²°ë¡  ë„ì¶œ",
                "steps": 3,
                "template": [
                    {
                        "module": "LLM_Agent",
                        "action": "__call__",
                        "role": "ì „ë¬¸ê°€ 1"
                    },
                    {
                        "module": "LLM_Agent",
                        "action": "__call__",
                        "role": "ì „ë¬¸ê°€ 2"
                    },
                    {
                        "module": "LLM_Agent",
                        "action": "aggregate_responses",
                        "role": "ì¢…í•© ë¶„ì„ê°€"
                    }
                ]
            },
            {
                "name": "ì˜¤ë””ì˜¤ ë¶„ì„ íŒŒì´í”„ë¼ì¸",
                "description": "ìŒì„± â†’ í…ìŠ¤íŠ¸ â†’ ë¶„ì„",
                "steps": 2,
                "template": [
                    {
                        "module": "Audio",
                        "action": "transcribe_audio"
                    },
                    {
                        "module": "LLM_Agent",
                        "action": "__call__"
                    }
                ]
            },
            {
                "name": "ë¶„ì„ í›„ Discord ì•Œë¦¼",
                "description": "ë¶„ì„ ê²°ê³¼ë¥¼ Discordë¡œ ì „ì†¡",
                "steps": 2,
                "template": [
                    {
                        "module": "LLM_Agent",
                        "action": "__call__"
                    },
                    {
                        "module": "Discord",
                        "action": "send_message"
                    }
                ]
            }
        ],
        "usage_guide": {
            "step_references": {
                "description": "ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ step_output_N í˜•íƒœë¡œ ì°¸ì¡°í•˜ì„¸ìš”",
                "examples": [
                    "step_output_1: ì²« ë²ˆì§¸ ë‹¨ê³„ì˜ ê²°ê³¼",
                    "step_output_2: ë‘ ë²ˆì§¸ ë‹¨ê³„ì˜ ê²°ê³¼"
                ]
            },
            "parameter_types": {
                "string": "ì¼ë°˜ í…ìŠ¤íŠ¸ ì…ë ¥",
                "textarea": "ì—¬ëŸ¬ ì¤„ í…ìŠ¤íŠ¸ ì…ë ¥",
                "select": "ë“œë¡­ë‹¤ìš´ ì„ íƒ",
                "multiselect": "ì—¬ëŸ¬ ì˜µì…˜ ì„ íƒ",
                "checkbox": "ì²´í¬ë°•ìŠ¤ (true/false)",
                "number": "ìˆ«ì ì…ë ¥",
                "url": "URL ì£¼ì†Œ",
                "password": "ë¹„ë°€ë²ˆí˜¸ ì…ë ¥",
                "array": "ë°°ì—´ í˜•íƒœì˜ ë°ì´í„°"
            }
        }
    }

class DynamicWorkflowRequest(BaseModel):
    name: str = Field(..., description="ì›Œí¬í”Œë¡œìš° ì´ë¦„")
    description: str = Field(default="", description="ì›Œí¬í”Œë¡œìš° ì„¤ëª…")
    steps: List[Dict[str, Any]] = Field(..., description="ë™ì ìœ¼ë¡œ êµ¬ì„±ëœ ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ë“¤")

@app.post("/build-workflow")
def build_dynamic_workflow(request: DynamicWorkflowRequest):
    """ì›¹ì—ì„œ êµ¬ì„±í•œ ë™ì  ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    try:
        workflow_steps = []

        for i, step_config in enumerate(request.steps, 1):
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if 'module' not in step_config or 'action' not in step_config:
                raise HTTPException(
                    status_code=400,
                    detail=f"ë‹¨ê³„ {i}ì— í•„ìˆ˜ í•„ë“œ (module, action)ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
                )

            # init_paramsì™€ params ë¶„ë¦¬
            init_params = step_config.get('init_params', {})
            params = step_config.get('params', {})

            # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ ì œê±°
            init_params = {k: v for k, v in init_params.items() if v not in [None, "", []]}
            params = {k: v for k, v in params.items() if v not in [None, "", []]}

            workflow_step = WorkflowStep(
                step=i,
                module=step_config['module'],
                action=step_config['action'],
                init_params=init_params,
                params=params
            )
            workflow_steps.append(workflow_step)

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        workflow_request = WorkflowRequest(workflow=workflow_steps)
        result = execute_workflow_endpoint(workflow_request)

        return {
            "workflow_info": {
                "name": request.name,
                "description": request.description,
                "total_steps": len(workflow_steps)
            },
            "execution_result": result
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë™ì  ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )

@app.post("/execute_workflow", response_model=WorkflowResponse)
def execute_workflow_endpoint(request: WorkflowRequest):
    """
    ğŸ¯ ë©”ì¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸

    JSONìœ¼ë¡œ ì •ì˜ëœ ì›Œí¬í”Œë¡œìš°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    ì˜ˆì‹œ ì›Œí¬í”Œë¡œìš°:
    ```json
    {
        "workflow": [
            {
                "step": 1,
                "module": "LLM_Agent",
                "action": "__call__",
                "init_params": {"model_name": "gemma3n", "provider": "ollama"},
                "params": {
                    "system_prompt": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                    "user_message": "ì•ˆë…•í•˜ì„¸ìš”!"
                }
            }
        ]
    }
    ```
    """
    if not request.workflow:
        raise HTTPException(status_code=400, detail="ì›Œí¬í”Œë¡œìš°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    try:
        # ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒì„± ë° ì‹¤í–‰
        orchestrator = WorkflowOrchestrator(request.workflow)
        result = orchestrator.execute()

        return WorkflowResponse(**result)

    except Exception as e:
        logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# === í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸ë“¤ (ë˜í¼) ===

class LegacyLLMRequest(BaseModel):
    system_prompt: str
    user_message: str

class LegacyMultiAgentRequest(BaseModel):
    system_prompt: str
    user_message: str
    task: str
    agents: List[dict]

class AgentConfig(BaseModel):
    name: str
    role: str
    task: str

class DynamicMultiAgentRequest(BaseModel):
    system_prompt: str
    user_message: str
    task: str
    agents: List[AgentConfig]

@app.post("/generate")
def generate_text_legacy(request: LegacyLLMRequest):
    """ê¸°ì¡´ /generate ì—”ë“œí¬ì¸íŠ¸ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    workflow = WorkflowRequest(workflow=[
        WorkflowStep(
            step=1,
            module="LLM_Agent",
            action="__call__",
            init_params={"model_name": "gemma3n", "provider": "ollama"},
            params={
                "system_prompt": request.system_prompt,
                "user_message": request.user_message
            }
        )
    ])

    result = execute_workflow_endpoint(workflow)

    if result.status == "success":
        return {"response": result.final_result}
    else:
        raise HTTPException(status_code=500, detail=result.error_message)

@app.post("/multi_agent")
def multi_agent_legacy(request: LegacyMultiAgentRequest):
    """ê¸°ì¡´ /multi_agent ì—”ë“œí¬ì¸íŠ¸ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    # ë™ì  ì—ì´ì „íŠ¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ì›Œí¬í”Œë¡œìš° ìƒì„±
    workflow_steps = []

    # ê°œë³„ ì—ì´ì „íŠ¸ë“¤ ì‹¤í–‰
    for i, agent_data in enumerate(request.agents, 1):
        if agent_data.get('name') and agent_data.get('role') and agent_data.get('task'):
            workflow_steps.append(WorkflowStep(
                step=i,
                module="LLM_Agent",
                action="__call__",
                init_params={"model_name": "gemma3n", "provider": "ollama"},
                params={
                    "system_prompt": agent_data['role'],
                    "user_message": request.user_message,
                    "task": agent_data['task']
                }
            ))

    # ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸ê°€ ëª¨ë“  ì‘ë‹µì„ ì¢…í•©
    final_step = len(workflow_steps) + 1
    workflow_steps.append(WorkflowStep(
        step=final_step,
        module="LLM_Agent",
        action="aggregate_responses",
        init_params={"model_name": "gemma3n", "provider": "ollama"},
        params={
            "system_prompt": request.system_prompt,
            "user_message": request.user_message,
            "task": request.task,
            "responses": [f"step_output_{i}" for i in range(1, len(workflow_steps) + 1)]
        }
    ))

    workflow = WorkflowRequest(workflow=workflow_steps)
    result = execute_workflow_endpoint(workflow)

    if result.status == "success":
        # ê¸°ì¡´ ì‘ë‹µ í¬ë§·ê³¼ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜
        individual_responses = []
        for i, agent_data in enumerate(request.agents, 1):
            if agent_data.get('name') and i in result.step_outputs:
                individual_responses.append({
                    "name": agent_data['name'],
                    "response": result.step_outputs[i]
                })

        return {
            "individual_responses": individual_responses,
            "final_response": result.final_result,
            "count": len(individual_responses)
        }
    else:
        raise HTTPException(status_code=500, detail=result.error_message)

@app.post("/run_dynamic_multi_agent")
def run_dynamic_multi_agent_legacy(request: DynamicMultiAgentRequest):
    """ê¸°ì¡´ /run_dynamic_multi_agent ì—”ë“œí¬ì¸íŠ¸ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
    # ì—ì´ì „íŠ¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ì›Œí¬í”Œë¡œìš° ìƒì„±
    workflow_steps = []

    # ê°œë³„ ì—ì´ì „íŠ¸ë“¤ ì‹¤í–‰
    for i, agent_config in enumerate(request.agents, 1):
        workflow_steps.append(WorkflowStep(
            step=i,
            module="LLM_Agent",
            action="__call__",
            init_params={"model_name": "gemma3n", "provider": "ollama"},
            params={
                "system_prompt": agent_config.role,
                "user_message": request.user_message,
                "task": agent_config.task
            }
        ))

    # ë§ˆìŠ¤í„° ì—ì´ì „íŠ¸ê°€ ëª¨ë“  ì‘ë‹µì„ ì¢…í•©
    final_step = len(workflow_steps) + 1
    workflow_steps.append(WorkflowStep(
        step=final_step,
        module="LLM_Agent",
        action="aggregate_responses",
        init_params={"model_name": "gemma3n", "provider": "ollama"},
        params={
            "system_prompt": request.system_prompt,
            "user_message": request.user_message,
            "task": request.task,
            "responses": [f"step_output_{i}" for i in range(1, len(workflow_steps) + 1)]
        }
    ))

    workflow = WorkflowRequest(workflow=workflow_steps)
    result = execute_workflow_endpoint(workflow)

    if result.status == "success":
        # ê¸°ì¡´ ì‘ë‹µ í¬ë§·ê³¼ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜
        individual_responses = []
        for i, agent_config in enumerate(request.agents, 1):
            if i in result.step_outputs:
                individual_responses.append({
                    "name": agent_config.name,
                    "response": result.step_outputs[i]
                })

        return {
            "individual_responses": individual_responses,
            "final_response": result.final_result,
            "count": len(individual_responses)
        }
    else:
        raise HTTPException(status_code=500, detail=result.error_message)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)